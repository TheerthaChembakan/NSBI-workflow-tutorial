{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing Notebook\n",
    "===\n",
    "\n",
    "Prepare data for the analysis. The raw data is downloaded from the FAIR Universe HiggsML challenge repository. Use the HiggsML package to download and process the dataset, followed by selections and saving to local cache.\n",
    "\n",
    "NB: This notebook is only to be run once to get the FAIR Universe dataset into `.root` ntuples. The rest of the workflow is independent of this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import yaml\n",
    "import uproot\n",
    "\n",
    "from utils import plot_kinematic_features\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import HiggsML\n",
    "from HiggsML.systematics import systematics\n",
    "hep.style.use(hep.style.ATLAS)\n",
    "\n",
    "from HiggsML.datasets import download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:44:45,732 - HiggsML.datasets     - INFO     - Handling as URL: https://zenodo.org/records/15131565/files/FAIR_Universe_HiggsML_data.zip\n",
      "2025-11-10 17:44:45,736 - HiggsML.datasets     - INFO     - Current working directory: /home/jsandesara_umass_edu/NSBI-workflow-tutorial/FAIR_universe_Higgs_tautau\n",
      "2025-11-10 17:44:45,965 - HiggsML.datasets     - INFO     - Total rows: 220099101\n",
      "2025-11-10 17:44:45,966 - HiggsML.datasets     - INFO     - Test size: 66029730\n"
     ]
    }
   ],
   "source": [
    "data = download_dataset(\"https://zenodo.org/records/15131565/files/FAIR_Universe_HiggsML_data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:44:54,525 - HiggsML.datasets     - INFO     - Selected train size: 53924279\n",
      "2025-11-10 17:51:04,104 - HiggsML.datasets     - INFO     - Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "data.load_train_set(train_size=0.35)\n",
    "df_training_full = data.get_train_set()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_processes_to_model = [\"htautau\", \"ztautau\", \"ttbar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diboson']\n"
     ]
    }
   ],
   "source": [
    "process_to_exclude = set(df_training_full[\"detailed_labels\"].unique()) - set(list_of_processes_to_model)\n",
    "process_to_exclude = list(process_to_exclude)\n",
    "print(process_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "detailed_labels\n",
       "ztautau    34425566\n",
       "htautau    17854072\n",
       "ttbar       1515091\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_process_exclusion = ~np.isin(df_training_full[\"detailed_labels\"], process_to_exclude)\n",
    "\n",
    "df_training_full = df_training_full[mask_process_exclusion].copy()\n",
    "df_training_full[\"detailed_labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the dataset, so all processes have equal entries\n",
    "\n",
    "# get the number of ttbar events (lowest)\n",
    "n_ttbar = df_training_full.loc[\n",
    "    df_training_full.detailed_labels=='ttbar'\n",
    "].shape[0]\n",
    "\n",
    "# Trim the other processes to match ttbar number, preserving event weight sums\n",
    "df_list = []\n",
    "for _, df_process in df_training_full.groupby('detailed_labels'):\n",
    "\n",
    "    weight_sum_orig = df_process.weights.sum()\n",
    "\n",
    "    df_sampled = df_process.sample(n = n_ttbar, random_state=42)\n",
    "\n",
    "    df_sampled['weights'] *= weight_sum_orig / df_sampled['weights'].sum()\n",
    "\n",
    "    df_list.append(df_sampled)\n",
    "    \n",
    "    del df_sampled\n",
    "\n",
    "df_training = pd.concat(df_list).reset_index(drop=True)\n",
    "del df_training_full, df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "syst_settings = {\n",
    "    'TES_up': {'tes': 1.02, 'seed': 42},\n",
    "    'TES_dn': {'tes': 0.98, 'seed': 42},\n",
    "    'JES_up': {'jes': 1.02, 'seed': 42},\n",
    "    'JES_dn': {'jes': 0.98, 'seed': 42}\n",
    "}\n",
    "\n",
    "\n",
    "dataset_dict = {}\n",
    "\n",
    "dataset_dict['nominal'] = systematics(\n",
    "        data_set = df_training,\n",
    "        dopostprocess=False\n",
    "        )\n",
    "\n",
    "for sample_name, syst_args in syst_settings.items():\n",
    "    dataset_dict[sample_name] = systematics(\n",
    "        data_set = df_training, \n",
    "        dopostprocess=False, \n",
    "        **syst_args\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_datasets = \"./saved_datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common analysis selections to remove low-stats regions\n",
    "selections = \"DER_mass_transverse_met_lep <= 250.0 and \\\n",
    "            DER_mass_vis <= 500.0 \\\n",
    "            and DER_sum_pt <= 1000 and \\\n",
    "            DER_pt_tot <= 250 and \\\n",
    "            DER_deltar_had_lep <= 4.5 and \\\n",
    "            DER_pt_h <= 400 and \\\n",
    "            DER_pt_ratio_lep_had <= 9.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset_dict.keys():\n",
    "\n",
    "    # Write to ROOT TTree\n",
    "    with uproot.recreate(f\"{saved_datasets}dataset_{sample}.root\") as ntuple:\n",
    "\n",
    "        for process in list_of_processes_to_model:\n",
    "\n",
    "            df = dataset_dict[sample]\n",
    "            \n",
    "            df_process = df[df[\"detailed_labels\"] == process].copy()\n",
    "\n",
    "            df_process = df_process.query(selections).copy()\n",
    "\n",
    "            columns_to_keep = df_process.columns.tolist()\n",
    "\n",
    "            columns_to_keep = list(set(columns_to_keep) - set([\"detailed_labels\"]))\n",
    "\n",
    "            arrays = {col: df_process[col].to_numpy() for col in columns_to_keep}\n",
    "\n",
    "            ntuple[f\"tree_{process}\"] = arrays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pixi: nsbi-env-gpu-new-new-new)",
   "language": "python",
   "name": "nsbi-env-gpu-ne-neww"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
